\documentclass{beamer}
\input{slides/_preamble_slides}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{tikz}

\title{Álgebra Lineal}
\subtitle{Clase Magistral}
\author{Jordan Blancas}
\date{\today}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Contenido}
  \tableofcontents
\end{frame}

% ========================================
% SECCIÓN 1: Bases, Dimensiones y Espacios Generados
% ========================================
\section{Bases, Dimensiones y Espacios Generados}

\begin{frame}{Espacios Vectoriales}
  \begin{block}{Definición}
    Un \textbf{espacio vectorial} $V$ sobre un campo $\mathbb{F}$ es un conjunto con dos operaciones:
    \begin{itemize}
      \item Suma de vectores: $+: V \times V \to V$
      \item Multiplicación por escalar: $\cdot: \mathbb{F} \times V \to V$
    \end{itemize}
    que satisfacen 8 axiomas (asociatividad, conmutatividad, etc.)
  \end{block}
  
  \begin{examples}
    \begin{itemize}
      \item $\mathbb{R}^n$ con suma y multiplicación estándar
      \item Polinomios $P_n(\mathbb{R})$ de grado $\leq n$
      \item Matrices $M_{m \times n}(\mathbb{R})$
    \end{itemize}
  \end{examples}
\end{frame}

\begin{frame}{Combinación Lineal}
  \begin{block}{Definición}
    Sea $V$ un espacio vectorial y $\{v_1, v_2, \ldots, v_k\} \subset V$. Un vector $v \in V$ es una \textbf{combinación lineal} de $\{v_1, \ldots, v_k\}$ si:
    $$v = \alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_k v_k$$
    donde $\alpha_1, \alpha_2, \ldots, \alpha_k \in \mathbb{F}$
  \end{block}
  
  \begin{example}
    En $\mathbb{R}^3$: El vector $(5, 1, 7)$ es combinación lineal de $(1,0,1)$ y $(2,1,3)$:
    $$(5, 1, 7) = 1 \cdot (1,0,1) + 2 \cdot (2,1,3)$$
  \end{example}
\end{frame}

\begin{frame}{Espacio Generado}
  \begin{block}{Definición}
    El \textbf{espacio generado} (o span) por un conjunto $S = \{v_1, v_2, \ldots, v_k\} \subset V$ es:
    $$\text{span}(S) = \left\{ \sum_{i=1}^{k} \alpha_i v_i \,:\, \alpha_i \in \mathbb{F} \right\}$$
    Es decir, el conjunto de todas las combinaciones lineales de elementos de $S$.
  \end{block}
  
  \begin{block}{Propiedades}
    \begin{itemize}
      \item $\text{span}(S)$ es un subespacio vectorial de $V$
      \item $S \subseteq \text{span}(S)$
      \item Es el subespacio más pequeño que contiene a $S$
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}{Dependencia e Independencia Lineal}
  \begin{block}{Definición: Independencia Lineal}
    Un conjunto $\{v_1, \ldots, v_k\}$ es \textbf{linealmente independiente} (L.I.) si:
    $$\alpha_1 v_1 + \cdots + \alpha_k v_k = 0 \implies \alpha_1 = \cdots = \alpha_k = 0$$
  \end{block}
  
  \begin{block}{Dependencia Lineal}
    Si no es L.I., es \textbf{linealmente dependiente} (L.D.): existe al menos un coeficiente no nulo tal que la combinación lineal da cero.
  \end{block}
  
  \begin{example}
    En $\mathbb{R}^3$:
    \begin{itemize}
      \item $\{(1,0,0), (0,1,0), (0,0,1)\}$ es L.I.
      \item $\{(1,2,3), (2,4,6)\}$ es L.D. (el segundo es múltiplo del primero)
    \end{itemize}
  \end{example}
\end{frame}

\begin{frame}{Base de un Espacio Vectorial}
  \begin{block}{Definición}
    Una \textbf{base} $\mathcal{B}$ de un espacio vectorial $V$ es un conjunto de vectores que cumple:
    \begin{enumerate}
      \item Es linealmente independiente
      \item Genera todo el espacio: $\text{span}(\mathcal{B}) = V$
    \end{enumerate}
  \end{block}
  
  \begin{block}{Propiedad Fundamental}
    Todo vector $v \in V$ se puede escribir de manera \textbf{única} como combinación lineal de los vectores de la base.
  \end{block}
  
  \begin{example}
    Base canónica de $\mathbb{R}^3$:
    $$\mathcal{B} = \{(1,0,0), (0,1,0), (0,0,1)\}$$
  \end{example}
\end{frame}

\begin{frame}{Dimensión}
  \begin{block}{Teorema}
    Todas las bases de un espacio vectorial $V$ tienen el mismo número de elementos.
  \end{block}
  
  \begin{block}{Definición: Dimensión}
    La \textbf{dimensión} de un espacio vectorial $V$, denotada $\dim(V)$, es el número de vectores en cualquier base de $V$.
  \end{block}
  
  \begin{examples}
    \begin{itemize}
      \item $\dim(\mathbb{R}^n) = n$
      \item $\dim(P_n(\mathbb{R})) = n + 1$ (polinomios de grado $\leq n$)
      \item $\dim(M_{m \times n}(\mathbb{R})) = m \cdot n$
      \item $\dim(\{0\}) = 0$ (espacio trivial)
    \end{itemize}
  \end{examples}
\end{frame}

\begin{frame}{Teorema de la Dimensión para Subespacios}
  \begin{theorem}
    Sea $V$ un espacio vectorial de dimensión finita y $W$ un subespacio de $V$. Entonces:
    $$\dim(W) \leq \dim(V)$$
    y la igualdad se cumple si y solo si $W = V$.
  \end{theorem}
  
  \begin{corollary}
    En $\mathbb{R}^n$:
    \begin{itemize}
      \item Cualquier conjunto de $n$ vectores L.I. forma una base
      \item Cualquier conjunto de más de $n$ vectores es L.D.
      \item Un subespacio de dimensión $k < n$ es un "objeto" de dimensión menor
    \end{itemize}
  \end{corollary}
\end{frame}

% ========================================
% SECCIÓN 2: Transformaciones Lineales
% ========================================
\section{Transformaciones Lineales, Núcleo e Imagen}

\begin{frame}{Transformación Lineal}
  \begin{block}{Definición}
    Sean $V$ y $W$ espacios vectoriales sobre $\mathbb{F}$. Una función $T: V \to W$ es una \textbf{transformación lineal} si:
    \begin{enumerate}
      \item $T(u + v) = T(u) + T(v)$ para todo $u, v \in V$
      \item $T(\alpha v) = \alpha T(v)$ para todo $\alpha \in \mathbb{F}$ y $v \in V$
    \end{enumerate}
  \end{block}
  
  \begin{block}{Equivalentemente}
    $T(\alpha u + \beta v) = \alpha T(u) + \beta T(v)$ para todo $u, v \in V$ y $\alpha, \beta \in \mathbb{F}$
  \end{block}
\end{frame}

\begin{frame}{Ejemplos de Transformaciones Lineales}
  \begin{examples}
    \begin{enumerate}
      \item \textbf{Identidad}: $I: V \to V$, $I(v) = v$
      
      \item \textbf{Transformación cero}: $0: V \to W$, $0(v) = 0_W$
      
      \item \textbf{Multiplicación por matriz}: $T: \mathbb{R}^n \to \mathbb{R}^m$
      $$T(x) = Ax$$
      donde $A \in M_{m \times n}(\mathbb{R})$
      
      \item \textbf{Derivada}: $D: P_n(\mathbb{R}) \to P_{n-1}(\mathbb{R})$
      $$D(p(x)) = p'(x)$$
      
      \item \textbf{Integral}: $I: C[a,b] \to \mathbb{R}$
      $$I(f) = \int_a^b f(x) \, dx$$
    \end{enumerate}
  \end{examples}
\end{frame}

\begin{frame}{Propiedades de Transformaciones Lineales}
  \begin{theorem}
    Sea $T: V \to W$ una transformación lineal. Entonces:
    \begin{enumerate}
      \item $T(0_V) = 0_W$
      \item $T(-v) = -T(v)$ para todo $v \in V$
      \item $T$ preserva combinaciones lineales:
      $$T\left(\sum_{i=1}^k \alpha_i v_i\right) = \sum_{i=1}^k \alpha_i T(v_i)$$
    \end{enumerate}
  \end{theorem}
  
  \begin{alertblock}{Importante}
    Una transformación lineal queda completamente determinada por su acción sobre una base de $V$.
  \end{alertblock}
\end{frame}

\begin{frame}{Núcleo (Kernel)}
  \begin{block}{Definición}
    El \textbf{núcleo} (o kernel) de una transformación lineal $T: V \to W$ es:
    $$\ker(T) = \{v \in V \,:\, T(v) = 0_W\}$$
    Es el conjunto de todos los vectores que $T$ mapea al vector cero.
  \end{block}
  
  \begin{theorem}
    $\ker(T)$ es un subespacio vectorial de $V$.
  \end{theorem}
  
  \begin{example}
    Para $T: \mathbb{R}^3 \to \mathbb{R}^2$, $T(x,y,z) = (x+y, 2x+2y)$:
    $$\ker(T) = \{(x,y,z) : x+y=0\} = \{(-t, t, s) : t, s \in \mathbb{R}\}$$
    Es un plano que pasa por el origen.
  \end{example}
\end{frame}

\begin{frame}{Imagen (Range)}
  \begin{block}{Definición}
    La \textbf{imagen} (o rango) de una transformación lineal $T: V \to W$ es:
    $$\text{Im}(T) = \{w \in W \,:\, \exists v \in V \text{ tal que } T(v) = w\}$$
    Es el conjunto de todos los vectores en $W$ que son imagen de algún vector en $V$.
  \end{block}
  
  \begin{theorem}
    $\text{Im}(T)$ es un subespacio vectorial de $W$.
  \end{theorem}
  
  \begin{block}{Observación}
    Si $\{v_1, \ldots, v_n\}$ es una base de $V$, entonces:
    $$\text{Im}(T) = \text{span}\{T(v_1), \ldots, T(v_n)\}$$
  \end{block}
\end{frame}

\begin{frame}{Inyectividad y Sobreyectividad}
  \begin{block}{Transformación Inyectiva}
    $T$ es \textbf{inyectiva} (uno a uno) si:
    $$T(u) = T(v) \implies u = v$$
    
    \textbf{Equivalentemente}: $\ker(T) = \{0\}$
  \end{block}
  
  \begin{block}{Transformación Sobreyectiva}
    $T$ es \textbf{sobreyectiva} (onto) si:
    $$\text{Im}(T) = W$$
    
    Es decir, todo vector en $W$ es imagen de algún vector en $V$.
  \end{block}
  
  \begin{block}{Isomorfismo}
    $T$ es un \textbf{isomorfismo} si es inyectiva y sobreyectiva (biyectiva).
  \end{block}
\end{frame}

\begin{frame}{Teorema de la Dimensión (Nullity-Rank)}
  \begin{theorem}[Teorema Fundamental]
    Sea $T: V \to W$ una transformación lineal, con $\dim(V) < \infty$. Entonces:
    $$\boxed{\dim(V) = \dim(\ker(T)) + \dim(\text{Im}(T))}$$
  \end{theorem}
  
  \begin{block}{Nomenclatura}
    \begin{itemize}
      \item \textbf{Nulidad}: $\text{nullity}(T) = \dim(\ker(T))$
      \item \textbf{Rango}: $\text{rank}(T) = \dim(\text{Im}(T))$
    \end{itemize}
    $$\boxed{\dim(V) = \text{nullity}(T) + \text{rank}(T)}$$
  \end{block}
  
  \begin{alertblock}{Consecuencia}
    Para $T: \mathbb{R}^n \to \mathbb{R}^m$ dada por $T(x) = Ax$:
    $$n = \text{nullity}(A) + \text{rank}(A)$$
  \end{alertblock}
\end{frame}

\begin{frame}{Matriz de una Transformación Lineal}
  \begin{block}{Representación Matricial}
    Sea $T: \mathbb{R}^n \to \mathbb{R}^m$ lineal. Existe una única matriz $A \in M_{m \times n}(\mathbb{R})$ tal que:
    $$T(x) = Ax \quad \forall x \in \mathbb{R}^n$$
  \end{block}
  
  \begin{block}{Construcción}
    Si $\{e_1, \ldots, e_n\}$ es la base canónica de $\mathbb{R}^n$, entonces la matriz $A$ tiene como columnas:
    $$A = [T(e_1) \,|\, T(e_2) \,|\, \cdots \,|\, T(e_n)]$$
  \end{block}
  
  \begin{example}
    Para $T: \mathbb{R}^2 \to \mathbb{R}^2$, rotación por $90°$:
    $$T(1,0) = (0,1), \quad T(0,1) = (-1,0)$$
    $$A = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}$$
  \end{example}
\end{frame}

% ========================================
% SECCIÓN 3: Tensores
% ========================================
\section{Tensores}

\begin{frame}{¿Qué es un Tensor?}
  \begin{block}{Intuición}
    Un tensor es una generalización multidimensional de:
    \begin{itemize}
      \item Escalar (tensor de orden 0): un número
      \item Vector (tensor de orden 1): lista de números
      \item Matriz (tensor de orden 2): tabla de números
      \item Tensor de orden $n$: arreglo $n$-dimensional de números
    \end{itemize}
  \end{block}
  
  \begin{block}{Definición Informal}
    Un tensor de orden $n$ (o rango $n$) en un espacio vectorial $V$ de dimensión $d$ es un objeto con $d^n$ componentes que se transforma de cierta manera bajo cambios de base.
  \end{block}
\end{frame}

\begin{frame}{Notación de Índices}
  \begin{block}{Convención}
    Un tensor se denota con índices superiores e inferiores:
    $$T^{i_1 i_2 \cdots i_p}_{j_1 j_2 \cdots j_q}$$
    \begin{itemize}
      \item Índices superiores: componentes \textbf{contravariantes}
      \item Índices inferiores: componentes \textbf{covariantes}
      \item El tensor es de tipo $(p, q)$
    \end{itemize}
  \end{block}
  
  \begin{examples}
    \begin{itemize}
      \item $v^i$: vector (tensor tipo $(1,0)$)
      \item $\omega_j$: covector o 1-forma (tensor tipo $(0,1)$)
      \item $A^i_j$: matriz o endomorfismo (tensor tipo $(1,1)$)
      \item $T^{ij}_{k}$: tensor tipo $(2,1)$
    \end{itemize}
  \end{examples}
\end{frame}

\begin{frame}{Producto Tensorial}
  \begin{block}{Definición}
    Sean $V$ y $W$ espacios vectoriales. El \textbf{producto tensorial} $V \otimes W$ es un nuevo espacio vectorial generado por símbolos $v \otimes w$ con $v \in V$, $w \in W$, sujeto a:
    \begin{align*}
      (v_1 + v_2) \otimes w &= v_1 \otimes w + v_2 \otimes w \\
      v \otimes (w_1 + w_2) &= v \otimes w_1 + v \otimes w_2 \\
      (\alpha v) \otimes w &= v \otimes (\alpha w) = \alpha(v \otimes w)
    \end{align*}
  \end{block}
  
  \begin{block}{Dimensión}
    Si $\dim(V) = n$ y $\dim(W) = m$, entonces:
    $$\dim(V \otimes W) = n \cdot m$$
  \end{block}
\end{frame}

\begin{frame}{Bases del Producto Tensorial}
  \begin{block}{Teorema}
    Si $\{e_1, \ldots, e_n\}$ es base de $V$ y $\{f_1, \ldots, f_m\}$ es base de $W$, entonces:
    $$\{e_i \otimes f_j \,:\, 1 \leq i \leq n, \, 1 \leq j \leq m\}$$
    es una base de $V \otimes W$.
  \end{block}
  
  \begin{example}
    En $\mathbb{R}^2 \otimes \mathbb{R}^2$, con bases canónicas:
    \begin{align*}
      &e_1 \otimes e_1, \quad e_1 \otimes e_2, \\
      &e_2 \otimes e_1, \quad e_2 \otimes e_2
    \end{align*}
    forman una base. Un tensor general es:
    $$T = T^{11} e_1 \otimes e_1 + T^{12} e_1 \otimes e_2 + T^{21} e_2 \otimes e_1 + T^{22} e_2 \otimes e_2$$
  \end{example}
\end{frame}

\begin{frame}{Tensores como Aplicaciones Multilineales}
  \begin{block}{Definición Alternativa}
    Un tensor de tipo $(p, q)$ sobre un espacio $V$ es una aplicación multilineal:
    $$T: \underbrace{V^* \times \cdots \times V^*}_{p \text{ veces}} \times \underbrace{V \times \cdots \times V}_{q \text{ veces}} \to \mathbb{F}$$
    donde $V^*$ es el espacio dual de $V$.
  \end{block}
  
  \begin{examples}
    \begin{itemize}
      \item Tensor $(0,0)$: escalar
      \item Tensor $(1,0)$: vector en $V$
      \item Tensor $(0,1)$: forma lineal (elemento de $V^*$)
      \item Tensor $(1,1)$: transformación lineal $V \to V$
      \item Tensor $(0,2)$: forma bilineal (ej: producto interno)
    \end{itemize}
  \end{examples}
\end{frame}

\begin{frame}{Contracción de Tensores}
  \begin{block}{Definición}
    La \textbf{contracción} es una operación que reduce el rango de un tensor igualando un índice superior con uno inferior y sumando:
    $$T^{i_1 \cdots i_p}_{j_1 \cdots j_q} \xrightarrow{\text{contracción}} S^{i_1 \cdots \hat{i}_k \cdots i_p}_{j_1 \cdots \hat{j}_\ell \cdots j_q} = \sum_{m} T^{i_1 \cdots m \cdots i_p}_{j_1 \cdots m \cdots j_q}$$
    donde $\hat{i}_k$ y $\hat{j}_\ell$ indican índices omitidos.
  \end{block}
  
  \begin{example}
    Para un tensor $T^i_j$ (matriz), la contracción da:
    $$T^i_i = \sum_{i=1}^n T^i_i = \text{tr}(T)$$
    ¡Es la traza de la matriz!
  \end{example}
\end{frame}

\begin{frame}{Operaciones con Tensores}
  \begin{block}{Operaciones Básicas}
    \begin{enumerate}
      \item \textbf{Suma}: Solo entre tensores del mismo tipo
      $$T + S = (T^{i_1\cdots i_p}_{j_1\cdots j_q} + S^{i_1\cdots i_p}_{j_1\cdots j_q})$$
      
      \item \textbf{Producto tensorial}: Aumenta el rango
      $$T \otimes S$$
      
      \item \textbf{Contracción}: Disminuye el rango
      
      \item \textbf{Transformación de coordenadas}: Bajo cambio de base $e_i' = A^j_i e_j$:
      $$T'^{i_1\cdots}_{j_1\cdots} = A^{i_1}_{k_1} \cdots (A^{-1})^{\ell_1}_{j_1} \cdots T^{k_1\cdots}_{\ell_1\cdots}$$
    \end{enumerate}
  \end{block}
\end{frame}

\begin{frame}{Aplicaciones de Tensores}
  \begin{block}{En Física}
    \begin{itemize}
      \item \textbf{Mecánica}: Tensor de inercia, tensor de esfuerzos
      \item \textbf{Relatividad General}: Tensor métrico $g_{\mu\nu}$, tensor de curvatura de Riemann $R^\rho_{\sigma\mu\nu}$
      \item \textbf{Electromagnetismo}: Tensor electromagnético $F^{\mu\nu}$
    \end{itemize}
  \end{block}
  
  \begin{block}{En Ingeniería y Ciencias}
    \begin{itemize}
      \item \textbf{Análisis de datos}: Descomposición tensorial (Tucker, PARAFAC)
      \item \textbf{Machine Learning}: Redes neuronales tensoriales, TensorFlow
      \item \textbf{Procesamiento de señales}: Análisis multilineal
      \item \textbf{Mecánica de medios continuos}: Tensor de deformación, tensor de Cauchy
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}{Ejemplo: Tensor Métrico}
  \begin{block}{Definición}
    En geometría diferencial, el \textbf{tensor métrico} $g$ es un tensor simétrico de tipo $(0,2)$ que define el producto interno:
    $$g: V \times V \to \mathbb{R}$$
    $$g(v, w) = g_{ij} v^i w^j$$
  \end{block}
  
  \begin{example}
    En $\mathbb{R}^3$ con coordenadas cartesianas:
    $$g_{ij} = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}$$
    
    En coordenadas esféricas $(r, \theta, \phi)$:
    $$g_{ij} = \begin{pmatrix} 1 & 0 & 0 \\ 0 & r^2 & 0 \\ 0 & 0 & r^2\sin^2\theta \end{pmatrix}$$
  \end{example}
\end{frame}

% ========================================
% CONCLUSIONES
% ========================================
\begin{frame}{Resumen}
  \begin{block}{Bases y Dimensiones}
    \begin{itemize}
      \item Base: conjunto L.I. que genera el espacio
      \item Dimensión: número de elementos en una base
      \item Toda base tiene el mismo número de elementos
    \end{itemize}
  \end{block}
  
  \begin{block}{Transformaciones Lineales}
    \begin{itemize}
      \item Preservan estructura: $T(\alpha u + \beta v) = \alpha T(u) + \beta T(v)$
      \item Núcleo: vectores que van al cero
      \item Imagen: vectores alcanzables
      \item Teorema de la dimensión: $\dim(V) = \dim(\ker T) + \dim(\text{Im} T)$
    \end{itemize}
  \end{block}
  
  \begin{block}{Tensores}
    \begin{itemize}
      \item Generalización de vectores y matrices
      \item Producto tensorial, contracción
      \item Fundamentales en física e ingeniería moderna
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}{Referencias}
  \begin{thebibliography}{99}
    \bibitem{strang} G. Strang, \textit{Linear Algebra and Its Applications}, 4th ed., Cengage Learning, 2006.
    
    \bibitem{axler} S. Axler, \textit{Linear Algebra Done Right}, 3rd ed., Springer, 2015.
    
    \bibitem{kolecki} J. Kolecki, \textit{An Introduction to Tensors for Students of Physics and Engineering}, NASA Technical Memorandum, 2002.
    
    \bibitem{wald} R. Wald, \textit{General Relativity}, University of Chicago Press, 1984.
  \end{thebibliography}
\end{frame}

\begin{frame}
  \centering
  \Huge ¿Preguntas?
  
  \vspace{1cm}
  
  \Large Gracias por su atención
\end{frame}

\end{document}
